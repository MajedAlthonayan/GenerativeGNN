{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. GPU.\n",
      "Cell done loading\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from MatrixVectorizer import MatrixVectorizer\n",
    "import random\n",
    "import optuna\n",
    "import numpy as np\n",
    "from adabound import AdaBound\n",
    "import pandas as pd\n",
    "from torch.nn import Module, ModuleList, Linear, ReLU, Sequential, Sigmoid\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from MatrixVectorizer import MatrixVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "import networkx as nx\n",
    "\n",
    "from model import AGSRNet \n",
    "from train import train, test\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "\n",
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA is available. GPU.\")\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        print(\"CUDA not available. CPU.\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "random_seed = 69 # nice\n",
    "set_seeds(random_seed)\n",
    "device = get_device()\n",
    "\n",
    "print(\"Cell done loading\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def add_random_edges(adj_matrix, num_edges):\n",
    "    \"\"\"\n",
    "    Randomly adds a given number of edges to the adjacency matrix of the graph.\n",
    "\n",
    "    Args:\n",
    "    adj_matrix (torch.Tensor): The adjacency matrix of the graph.\n",
    "    num_edges (int): The number of edges to add.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The augmented adjacency matrix.\n",
    "    \"\"\"\n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    new_edges = 0\n",
    "    new_adj_matrix = adj_matrix.clone()\n",
    "\n",
    "    while new_edges < num_edges:\n",
    "        i = random.randint(0, num_nodes - 1)\n",
    "        j = random.randint(0, num_nodes - 1)\n",
    "\n",
    "        if i != j and new_adj_matrix[i, j] == 0:\n",
    "            new_adj_matrix[i, j] = new_adj_matrix[j, i] = 1\n",
    "            new_edges += 1\n",
    "\n",
    "    return new_adj_matrix\n",
    "\n",
    "def drop_random_edges(adj_matrix, num_edges):\n",
    "    \"\"\"\n",
    "    Randomly removes a given number of edges from the adjacency matrix of the graph.\n",
    "\n",
    "    Args:\n",
    "    adj_matrix (torch.Tensor): The adjacency matrix of the graph.\n",
    "    num_edges (int): The number of edges to remove.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The augmented adjacency matrix.\n",
    "    \"\"\"\n",
    "    edges = [(i, j) for i in range(adj_matrix.shape[0]) for j in range(i) if adj_matrix[i, j] > 0]\n",
    "    edges_to_drop = random.sample(edges, num_edges)\n",
    "\n",
    "    new_adj_matrix = adj_matrix.clone()\n",
    "    for (i, j) in edges_to_drop:\n",
    "        new_adj_matrix[i, j] = new_adj_matrix[j, i] = 0\n",
    "\n",
    "    return new_adj_matrix\n",
    "\n",
    "def shuffle_node_features(node_features):\n",
    "    \"\"\"\n",
    "    Randomly shuffles the node features of the graph.\n",
    "\n",
    "    Args:\n",
    "    node_features (torch.Tensor): The node features.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The node features with shuffled feature positions.\n",
    "    \"\"\"\n",
    "    num_features = node_features.shape[1]\n",
    "    perm = torch.randperm(num_features)\n",
    "    new_features = node_features[:, perm]\n",
    "\n",
    "    return new_features\n",
    "\n",
    "def augment_data(adj_matrices, num_augmentations):\n",
    "    \"\"\"\n",
    "    Apply a series of augmentations to the adjacency matrices.\n",
    "\n",
    "    Args:\n",
    "    adj_matrices (list of torch.Tensor): A list of adjacency matrices.\n",
    "    num_augmentations (int): Number of augmentations to apply.\n",
    "\n",
    "    Returns:\n",
    "    list of torch.Tensor: List of augmented adjacency matrices.\n",
    "    \"\"\"\n",
    "    augmented_adjs = []\n",
    "\n",
    "    for adj in adj_matrices:\n",
    "        for _ in range(num_augmentations):\n",
    "            # Randomly add or remove edges\n",
    "            num_edges_to_change = random.randint(1, 20)\n",
    "            if random.random() > 0.5:\n",
    "                new_adj = add_random_edges(adj, num_edges_to_change)\n",
    "            else:\n",
    "                new_adj = drop_random_edges(adj, num_edges_to_change)\n",
    "\n",
    "            augmented_adjs.append(new_adj)\n",
    "\n",
    "    return augmented_adjs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell done loading\n"
     ]
    }
   ],
   "source": [
    "class SymmetricMatrixVectorizer:\n",
    "    \"\"\"Handles vectorization and devectorization of symmetric matrices.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectorizer = MatrixVectorizer()\n",
    "\n",
    "    def vectorize(self, matrix):\n",
    "        \"\"\"Vectorize a symmetric matrix\"\"\"\n",
    "        matrix_np = matrix.numpy()  # Convert to NumPy array for vectorization\n",
    "        return self.vectorizer.vectorize(matrix_np)\n",
    "\n",
    "    def devectorize(self, vector, size):\n",
    "        \"\"\"Devectorize into a symmetric matrix\"\"\"\n",
    "        if isinstance(vector, torch.Tensor):\n",
    "            vector = vector.numpy()  # Ensure the vector is a NumPy array\n",
    "        matrix_np = self.vectorizer.anti_vectorize(vector, size)\n",
    "        return torch.tensor(matrix_np, dtype=torch.float)\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load dataset from a CSV file.\"\"\"\n",
    "    return pd.read_csv(file_path).values\n",
    "\n",
    "def tensor_conversion(data, dimension, vectorizer):\n",
    "    \"\"\"Convert list of matrices to a PyTorch tensor.\"\"\"\n",
    "    tensor_list = [vectorizer.devectorize(x, dimension) for x in data]\n",
    "    tensor_data = torch.stack(tensor_list)  # Use torch.stack to create a 3D tensor from the list\n",
    "    return tensor_data\n",
    "\n",
    "\n",
    "def normalisation(adj):\n",
    "    # Add self-loops to the adjacency matrix\n",
    "    adj_with_self_loops = adj + torch.eye(adj.shape[0], device=adj.device)\n",
    "    \n",
    "    # Calculate the degree matrix (with added self-loops), and then compute its inverse square root\n",
    "    D_hat_inv_sqrt = torch.diag(1.0 / torch.sqrt(torch.sum(adj_with_self_loops, axis=0)))\n",
    "    \n",
    "    # Apply normalization\n",
    "    adj_norm = D_hat_inv_sqrt @ adj_with_self_loops @ D_hat_inv_sqrt\n",
    "    \n",
    "    return adj_norm\n",
    "\n",
    "\n",
    "# Initialize vectorizer\n",
    "vectorizer = SymmetricMatrixVectorizer()\n",
    "\n",
    "# Load the data\n",
    "lr_train = load_data('data/lr_train.csv')\n",
    "hr_train = load_data('data/hr_train.csv')\n",
    "lr_test = load_data('data/lr_test.csv')\n",
    "\n",
    "# Convert datasets to tensors- not used currently since vctoisation happens later\n",
    "lr_train_devectorized = tensor_conversion(lr_train, 160, vectorizer)\n",
    "hr_train_devectorized = tensor_conversion(hr_train, 268, vectorizer)\n",
    "lr_test_devectorized = tensor_conversion(lr_test, 160, vectorizer)\n",
    "\n",
    "print(\"Cell done loading\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, features, labels=None, device='cuda'):\n",
    "        \"\"\"\n",
    "        Initializes the dataset.\n",
    "\n",
    "        Args:\n",
    "            features (Tensor): The features of the dataset.\n",
    "            labels (Tensor, optional): The labels of the dataset. Defaults to None.\n",
    "            device (str, optional): The device to which the tensors will be transferred. Defaults to 'cpu'.\n",
    "        \"\"\"\n",
    "        self.features = features.clone().detach().to(device).float()\n",
    "        self.labels = None if labels is None else labels.clone().detach().to(device).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of items in the dataset.\"\"\"\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the feature and label at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the item.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the feature and label at the specified index.\n",
    "        \"\"\"\n",
    "        label = self.labels[idx] if self.labels is not None else None\n",
    "        return self.features[idx], label\n",
    "\n",
    "def adabound_optimizer(params, lr=0.001, final_lr=0.1):\n",
    "    \"\"\"Create and return an AdaBound optimizer.\"\"\"\n",
    "    return AdaBound(params, lr=lr, final_lr=final_lr)\n",
    "\n",
    "\n",
    "def prepare_datasets(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Prepare training and validation datasets.\"\"\"\n",
    "    if X_val is None:\n",
    "        return {'train': Data(X_train, y_train)}\n",
    "    \n",
    "    return {\n",
    "        'train': Data(X_train, y_train),\n",
    "        'val': Data(X_val, y_val)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGSR:\n",
    "    def __init__(self):\n",
    "        self.epochs = 200\n",
    "        self.lr = 0.0001\n",
    "        self.lmbda = 0.1\n",
    "        self.lr_dim = 160\n",
    "        self.hr_dim = 320\n",
    "        self.hidden_dim = 320\n",
    "        self.padding = 26\n",
    "        self.mean_dense = 0.\n",
    "        self.std_dense = 0.01\n",
    "        self.mean_gaussian = 0.\n",
    "        self.std_gaussian = 0.1\n",
    "        \n",
    "        # Model setup\n",
    "        kernel_sizes = [0.9, 0.7, 0.6, 0.5]\n",
    "        self.model = AGSRNet(kernel_sizes, self)\n",
    "        self.model_path = 'data/agsr_model.pth'  \n",
    "        \n",
    "    def save_model(self):\n",
    "        \"\"\"Save the model to the specified path.\"\"\"\n",
    "        torch.save(self.model.state_dict(), self.model_path)\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load the model from the specified path.\"\"\"\n",
    "        self.model.load_state_dict(torch.load(self.model_path))\n",
    "\n",
    "    def train(self, lr_vectors, hr_vectors):\n",
    "        \"\"\"Train the model and save it.\"\"\"\n",
    "        train(self.model, lr_vectors, hr_vectors, self)\n",
    "        self.save_model()\n",
    "\n",
    "    def predict(self, lr_vectors):\n",
    "        \"\"\"Load the model and predict high-resolution vectors from low-resolution inputs.\"\"\"\n",
    "        self.load_model()\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            predicted_hr_vectors = test(self.model, lr_vectors, self)\n",
    "        return predicted_hr_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: \n",
      "Discriminator(\n",
      "  (dense_1): Dense()\n",
      "  (relu_1): ReLU()\n",
      "  (dense_2): Dense()\n",
      "  (relu_2): ReLU()\n",
      "  (dense_3): Dense()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch:  0 Loss:  9.213129765815562 Error:  825.8791690138546 %\n",
      "Epoch:  1 Loss:  3.293942533097826 Error:  6.131950846394977 %\n",
      "Epoch:  2 Loss:  5.578815311998935 Error:  5.92905773638605 %\n",
      "Epoch:  3 Loss:  6.9001510422509 Error:  5.8304090321332485 %\n",
      "Epoch:  4 Loss:  7.8661360783619925 Error:  5.669938605110925 %\n",
      "Epoch:  5 Loss:  9.089051624676129 Error:  5.556640505522221 %\n",
      "Epoch:  6 Loss:  10.208107681961748 Error:  5.460709145477226 %\n",
      "Epoch:  7 Loss:  11.131424706261438 Error:  5.365167591754381 %\n",
      "Epoch:  8 Loss:  11.826470589852548 Error:  5.266634022464623 %\n",
      "Epoch:  9 Loss:  12.286136592830623 Error:  5.1577434816339 %\n",
      "Epoch:  10 Loss:  12.815865619762524 Error:  5.058579472405417 %\n",
      "Epoch:  11 Loss:  13.369807569830268 Error:  4.9864148469390095 %\n",
      "Epoch:  12 Loss:  13.895331073451686 Error:  4.920382366456964 %\n",
      "Epoch:  13 Loss:  14.160236856958887 Error:  4.839265163618702 %\n",
      "Epoch:  14 Loss:  14.286350774335432 Error:  4.750821034650545 %\n",
      "Epoch:  15 Loss:  14.452825941481033 Error:  4.660025670251867 %\n",
      "Epoch:  16 Loss:  14.941290984282622 Error:  4.601345080378893 %\n",
      "Epoch:  17 Loss:  15.421019571321505 Error:  4.55397127347218 %\n",
      "Epoch:  18 Loss:  15.809303446932956 Error:  4.508250819200331 %\n",
      "Epoch:  19 Loss:  15.88387460966368 Error:  4.439748675973566 %\n",
      "Epoch:  20 Loss:  16.253231916341697 Error:  4.396757428106424 %\n",
      "Epoch:  21 Loss:  16.427747176574158 Error:  4.342618804458563 %\n",
      "Epoch:  22 Loss:  16.75002336072492 Error:  4.297214944486146 %\n",
      "Epoch:  23 Loss:  17.203805803178668 Error:  4.262957898144786 %\n",
      "Epoch:  24 Loss:  17.444921751280088 Error:  4.218603115159649 %\n",
      "Epoch:  25 Loss:  17.853593130369443 Error:  4.18250337410886 %\n",
      "Epoch:  26 Loss:  18.278801634504987 Error:  4.152050396209365 %\n",
      "Epoch:  27 Loss:  18.63615130519008 Error:  4.121029580028744 %\n",
      "Epoch:  28 Loss:  18.862023568368173 Error:  4.081823012313327 %\n",
      "Epoch:  29 Loss:  19.08128596640922 Error:  4.040133656078094 %\n",
      "Epoch:  30 Loss:  19.52026639543138 Error:  4.00857257883291 %\n",
      "Epoch:  31 Loss:  19.922765989561338 Error:  3.9750103343714462 %\n",
      "Epoch:  32 Loss:  20.42679682723037 Error:  3.947665320860373 %\n",
      "Epoch:  33 Loss:  20.758049750113273 Error:  3.9153695358215153 %\n",
      "Epoch:  34 Loss:  21.012777010599773 Error:  3.8789100877873532 %\n",
      "Epoch:  35 Loss:  21.535775966472453 Error:  3.853215200004277 %\n",
      "Epoch:  36 Loss:  21.98940540863587 Error:  3.82777405147617 %\n",
      "Epoch:  37 Loss:  22.3007227923419 Error:  3.8032057216844044 %\n",
      "Epoch:  38 Loss:  22.420369655162364 Error:  3.78022913460259 %\n",
      "Epoch:  39 Loss:  22.674675907100642 Error:  3.754003033847422 %\n",
      "Epoch:  40 Loss:  22.076993976627385 Error:  3.719956145898716 %\n",
      "Epoch:  41 Loss:  22.184035369941782 Error:  3.6851200048585198 %\n",
      "Epoch:  42 Loss:  21.835568015639847 Error:  3.659185859466995 %\n",
      "Epoch:  43 Loss:  21.698061951645858 Error:  3.6348239577434085 %\n",
      "Epoch:  44 Loss:  21.55346717490806 Error:  3.6077360899464503 %\n",
      "Epoch:  45 Loss:  21.28712712537061 Error:  3.5744422375484626 %\n",
      "Epoch:  46 Loss:  20.72783829285218 Error:  3.539732808398234 %\n",
      "Epoch:  47 Loss:  20.16733270937258 Error:  3.5036275280756994 %\n",
      "Epoch:  48 Loss:  19.942863103505726 Error:  3.478469375755873 %\n",
      "Epoch:  49 Loss:  19.72736510714969 Error:  3.451374409770643 %\n",
      "Epoch:  50 Loss:  19.50836920523429 Error:  3.426558420330555 %\n",
      "Epoch:  51 Loss:  18.92462358388815 Error:  3.394170761578255 %\n",
      "Epoch:  52 Loss:  18.774065198125065 Error:  3.370767492834512 %\n",
      "Epoch:  53 Loss:  18.656852790901254 Error:  3.3424792039367532 %\n",
      "Epoch:  54 Loss:  18.53605602883004 Error:  3.3169945450247944 %\n",
      "Epoch:  55 Loss:  18.57085764085924 Error:  3.2929791557090775 %\n",
      "Epoch:  56 Loss:  18.53539058754036 Error:  3.271418848486097 %\n",
      "Epoch:  57 Loss:  18.523081727930016 Error:  3.2509361687409983 %\n",
      "Epoch:  58 Loss:  18.441038819046707 Error:  3.2275667951047957 %\n",
      "Epoch:  59 Loss:  18.310629956357115 Error:  3.2059083080238047 %\n",
      "Epoch:  60 Loss:  17.897768433029587 Error:  3.183089389658726 %\n",
      "Epoch:  61 Loss:  16.469363985834896 Error:  3.1504615805707537 %\n",
      "Epoch:  62 Loss:  15.107084798383283 Error:  3.1224523293408186 %\n",
      "Epoch:  63 Loss:  13.735615206194353 Error:  3.106357759653448 %\n",
      "Epoch:  64 Loss:  11.019664931941676 Error:  3.095361140665707 %\n",
      "Epoch:  65 Loss:  9.720362048965317 Error:  3.1375230080238334 %\n",
      "Epoch:  66 Loss:  8.364876356210795 Error:  3.1631812637856416 %\n",
      "Epoch:  67 Loss:  7.675385672766883 Error:  3.1377983573186503 %\n",
      "Epoch:  68 Loss:  6.69312251580728 Error:  3.150270984986344 %\n",
      "Epoch:  69 Loss:  5.649330566595267 Error:  3.092650874509468 %\n",
      "Epoch:  70 Loss:  5.380987899797457 Error:  3.039294130563199 %\n",
      "Epoch:  71 Loss:  5.547670312829919 Error:  3.0047326289989926 %\n",
      "Epoch:  72 Loss:  5.7492647256937115 Error:  2.9562602104904414 %\n",
      "Epoch:  73 Loss:  5.956799077558088 Error:  2.9150614280018723 %\n",
      "Epoch:  74 Loss:  6.196983049581717 Error:  2.8838750478383655 %\n",
      "Epoch:  75 Loss:  6.352403279897329 Error:  2.858630450324969 %\n",
      "Epoch:  76 Loss:  6.53158696492513 Error:  2.83343004784337 %\n",
      "Epoch:  77 Loss:  6.624242739634471 Error:  2.810862027779893 %\n",
      "Epoch:  78 Loss:  6.577507173692858 Error:  2.781878188588061 %\n",
      "Epoch:  79 Loss:  6.717679526354815 Error:  2.7587467468939386 %\n",
      "Epoch:  80 Loss:  6.736762519355293 Error:  2.7337369718798645 %\n",
      "Epoch:  81 Loss:  6.815896549740353 Error:  2.708876636382696 %\n",
      "Epoch:  82 Loss:  7.006913743577562 Error:  2.6835619715285732 %\n",
      "Epoch:  83 Loss:  7.193442860165158 Error:  2.663882870462027 %\n",
      "Epoch:  84 Loss:  7.251106391081938 Error:  2.637452368733582 %\n",
      "Epoch:  85 Loss:  7.443766581045614 Error:  2.615502552204841 %\n",
      "Epoch:  86 Loss:  7.646184152311033 Error:  2.592652979063558 %\n",
      "Epoch:  87 Loss:  7.771355813687986 Error:  2.569352649152279 %\n"
     ]
    }
   ],
   "source": [
    "agr_model = AGSR()\n",
    "\n",
    "# 3-Fold cross-validation setup\n",
    "low_resolution_images = np.array(lr_train)  \n",
    "high_resolution_images = np.array(hr_train)  \n",
    "k_fold_splitter = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "cross_validation_scores = []\n",
    "current_fold = 0\n",
    "\n",
    "for training_indices, testing_indices in k_fold_splitter.split(low_resolution_images):\n",
    "    current_fold += 1\n",
    "    print(f\"Fold {current_fold}: \")\n",
    "    low_res_train, low_res_test = low_resolution_images[training_indices], low_resolution_images[testing_indices]\n",
    "    high_res_train, high_res_test = high_resolution_images[training_indices], high_resolution_images[testing_indices]\n",
    "\n",
    "    fold_model = AGSR()\n",
    "    \n",
    "    train_input_matrices = [MatrixVectorizer.anti_vectorize(low_res_train[i, :], 160) for i in range(low_res_train.shape[0])]\n",
    "    test_input_matrices = [MatrixVectorizer.anti_vectorize(low_res_test[i, :], 160) for i in range(low_res_test.shape[0])]\n",
    "    train_output_matrices = [MatrixVectorizer.anti_vectorize(high_res_train[i, :], 268) for i in range(high_res_train.shape[0])]\n",
    "\n",
    "    train_input_matrices = np.array(train_input_matrices)\n",
    "    test_input_matrices = np.array(test_input_matrices)\n",
    "    train_output_matrices = np.array(train_output_matrices)\n",
    "\n",
    "    fold_model.train(train_input_matrices, train_output_matrices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KAGGLE CODE FOR AGSR\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def data_preprocessing(data_path):\n",
    "    \"\"\"Load data and cleanse by replacing negative and NaN values with 0.\"\"\"\n",
    "    data = pd.read_csv(data_path)\n",
    "    data = np.maximum(data, 0)  # Ensures all negative values are set to 0\n",
    "    data = np.nan_to_num(data)  # Replaces NaNs with 0 and returns a numpy array\n",
    "    return data\n",
    "\n",
    "def vectorize_data(data):\n",
    "    \"\"\"Vectorize the data using a MatrixVectorizer and process it for prediction.\"\"\"\n",
    "    vectorized_data = [MatrixVectorizer.anti_vectorize(row, 160) for row in data]\n",
    "    return np.array(vectorized_data)\n",
    "\n",
    "def process_predictions(predictions):\n",
    "    \"\"\"Vectorize predictions, flatten the array, and prepare submission DataFrame.\"\"\"\n",
    "    vectorized_predictions = np.array([MatrixVectorizer.vectorize(pred) for pred in predictions])\n",
    "    flattened_predictions = vectorized_predictions.flatten()\n",
    "    predictions_for_csv = pd.DataFrame({\n",
    "        'ID': np.arange(1, len(flattened_predictions) + 1),\n",
    "        'Predicted': flattened_predictions\n",
    "    })\n",
    "    return predictions_for_csv\n",
    "\n",
    "test_data_path = 'data/lr_test.csv'\n",
    "preprocessed_data = data_preprocessing(test_data_path)\n",
    "# need to vectorize for kaggle\n",
    "vectorized_test_data = vectorize_data(preprocessed_data)\n",
    "\n",
    "test_predictions_tmp = agr_model.predict(vectorized_test_data)\n",
    "predictions_for_csv = process_predictions(test_predictions_tmp)\n",
    "\n",
    "predictions_for_csv.to_csv('data/final_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
